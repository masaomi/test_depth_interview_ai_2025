# ========================================
# LM Studio Configuration Sample
# ========================================
# This is a sample .env file for using LM Studio as your LLM provider
# Copy this file to .env to use it:
#   cp env.lmstudio.example .env

# LLM Provider: Set to 'local' to use LM Studio
LLM_PROVIDER=local

# LM Studio Base URL
# Default LM Studio server runs at http://localhost:1234
# Make sure to include /v1 at the end for OpenAI-compatible API
LOCAL_LLM_BASE_URL=http://localhost:1234/v1

# Model Name
# Use the model name displayed in LM Studio
# Examples:
#   - TheBloke/Llama-2-7B-Chat-GGUF
#   - mistralai/Mistral-7B-Instruct-v0.2
#   - gpt-oss20B (if you have this custom model)
LOCAL_LLM_MODEL=gpt-oss20B

# API Key (Optional for LM Studio)
# LM Studio doesn't require authentication by default
# Leave this commented out unless your setup requires it
# LOCAL_LLM_API_KEY=dummy

# ========================================
# OpenAI Configuration (Not used when LLM_PROVIDER=local)
# ========================================
# You can keep your OpenAI key here for easy switching
# It will be ignored when LLM_PROVIDER=local is set
# OPENAI_API_KEY=sk-proj-your-key-here
# OPENAI_MODEL=gpt-4

# ========================================
# Setup Instructions
# ========================================
# 1. Install LM Studio from https://lmstudio.ai/
# 2. Download your desired model in LM Studio
# 3. Load the model in LM Studio
# 4. Start the local server:
#    - Go to "Local Server" tab in LM Studio
#    - Click "Start Server"
#    - Default port is 1234
# 5. Copy this file to .env:
#    cp env.lmstudio.example .env
# 6. Update LOCAL_LLM_MODEL to match your loaded model name
# 7. Run the development server:
#    npm run dev

# ========================================
# Admin Panel (Optional)
# ========================================
# Set a password to enable the Admin Panel UI. If unset, the
# Admin Panel is disabled and the button is hidden.
# ADMIN_PASSWORD=change-me

