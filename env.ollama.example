# ========================================
# Ollama Configuration Sample
# ========================================
# This is a sample .env file for using Ollama as your LLM provider
# Copy this file to .env to use it:
#   cp env.ollama.example .env

# LLM Provider: Set to 'local' to use Ollama
LLM_PROVIDER=local

# Ollama Base URL
# Default Ollama server runs at http://localhost:11434
# Make sure to include /v1 at the end for OpenAI-compatible API
LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# Model Name
# Replace with your model name in Ollama
# Examples:
#   - llama2
#   - llama3
#   - mistral
#   - codellama
#   - gpt-oss20B (if you have this custom model)
LOCAL_LLM_MODEL=llama2

# API Key (Optional for Ollama)
# Ollama doesn't require authentication by default
# Leave this commented out unless your setup requires it
# LOCAL_LLM_API_KEY=dummy

# ========================================
# OpenAI Configuration (Not used when LLM_PROVIDER=local)
# ========================================
# You can keep your OpenAI key here for easy switching
# It will be ignored when LLM_PROVIDER=local is set
# OPENAI_API_KEY=sk-proj-your-key-here
# OPENAI_MODEL=gpt-4

# ========================================
# Setup Instructions
# ========================================
# 1. Install Ollama from https://ollama.ai/
# 2. Pull your desired model:
#    ollama pull llama2
#    or
#    ollama pull llama3
# 3. Verify Ollama is running:
#    ollama list
#    curl http://localhost:11434/v1/models
# 4. Copy this file to .env:
#    cp env.ollama.example .env
# 5. Update LOCAL_LLM_MODEL to match your pulled model
# 6. Run the development server:
#    npm run dev

